<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Andreas Niekler, Gregor Wiedemann" />

<meta name="date" content="2020-10-07" />

<title>Tutorial 6: Topic Models</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/united.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/clipboard-1.7.1/clipboard.min.js"></script>
<link href="site_libs/primer-tooltips-1.4.0/build.css" rel="stylesheet" />
<link href="site_libs/klippy-0.0.0.9500/css/klippy.min.css" rel="stylesheet" />
<script src="site_libs/klippy-0.0.0.9500/js/klippy.min.js"></script>
<script src="site_libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<link href="site_libs/wordcloud2-0.0.1/wordcloud.css" rel="stylesheet" />
<script src="site_libs/wordcloud2-0.0.1/wordcloud2-all.js"></script>
<script src="site_libs/wordcloud2-0.0.1/hover.js"></script>
<script src="site_libs/wordcloud2-binding-0.2.1/wordcloud2.js"></script>
<link href="site_libs/ionicons-2.0.1/css/ionicons.min.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html"></a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Intro</a>
</li>
<li>
  <a href="Tutorial_1_Read_textdata.html">
    <span class="ion ion-android-bulb"></span>
     
    Tutorial 1
  </a>
</li>
<li>
  <a href="Tutorial_2_Web_crawling.html">
    <span class="ion ion-android-bulb"></span>
     
    Tutorial 2
  </a>
</li>
<li>
  <a href="Tutorial_3_Frequency.html">
    <span class="ion ion-android-bulb"></span>
     
    Tutorial 3
  </a>
</li>
<li>
  <a href="Tutorial_4_Term_extraction.html">
    <span class="ion ion-android-bulb"></span>
     
    Tutorial 4
  </a>
</li>
<li>
  <a href="Tutorial_5_Co-occurrence.html">
    <span class="ion ion-android-bulb"></span>
     
    Tutorial 5
  </a>
</li>
<li>
  <a href="Tutorial_6_Topic_Models.html">
    <span class="ion ion-android-bulb"></span>
     
    Tutorial 6
  </a>
</li>
<li>
  <a href="Tutorial_7_Klassifikation.html">
    <span class="ion ion-android-bulb"></span>
     
    Tutorial 7
  </a>
</li>
<li>
  <a href="Tutorial_8_NER_POS.html">
    <span class="ion ion-android-bulb"></span>
     
    Tutorial 8
  </a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Tutorial 6: Topic Models</h1>
<h4 class="author">Andreas Niekler, Gregor Wiedemann</h4>
<h4 class="date">2020-10-07</h4>

</div>


<p><script>
  addClassKlippyTo("pre.r, pre.markdown");
  addKlippy('left', 'top', 'auto', '1', 'Copy code', 'Copied!');
</script> This exercise demonstrates the use of topic models on a text corpus for the extraction of latent semantic contexts in the documents. In this exercise we will:</p>
<ol style="list-style-type: decimal">
<li>Read in and preprocess text data,</li>
<li>Calculate a topic model using the R package <em>topmicmodels</em> and analyze its results in more detail,</li>
<li>Visualize the results from the calculated model and</li>
<li>Select documents based on their topic composition.</li>
</ol>
<p>The process starts as usual with the reading of the corpus data. Change to your working directory, create a new R script, load the quanteda-package and define a few already known default variables.</p>
<pre class="r"><code># setwd(&quot;Your work directory&quot;)
options(stringsAsFactors = FALSE)
library(quanteda)
require(topicmodels)</code></pre>
<p>The 231 SOTU addresses are rather long documents. Documents lengths clearly affects the results of topic modeling. For very short texts (e.g. Twitter posts) or very long texts (e.g. books), it can make sense to concatenate/split single documents to receive longer/shorter textual units for modeling.</p>
<p>For the SOTU speeches for instance, we infer the model based on paragraphs instead of entire speeches. By manual inspection / qualitative inspection of the results you can check if this procedure yields better (interpretable) topics. In <code>sotu_paragraphs.csv</code>, we provide a paragraph separated version of the speeches.</p>
<p>For text preprocessing, we remove stopwords, since they tend to occur as “noise” in the estimated topics of the LDA model.</p>
<pre class="r"><code>textdata &lt;- read.csv(&quot;data/sotu.csv&quot;, sep = &quot;;&quot;, encoding = &quot;UTF-8&quot;)

sotu_corpus &lt;- corpus(textdata$text, docnames = textdata$doc_id)

# Build a dictionary of lemmas
lemma_data &lt;- read.csv(&quot;resources/baseform_en.tsv&quot;, encoding = &quot;UTF-8&quot;)

# extended stopword list
stopwords_extended &lt;- readLines(&quot;resources/stopwords_en.txt&quot;, encoding = &quot;UTF-8&quot;)

# Create a DTM (may take a while)
corpus_tokens &lt;- sotu_corpus %&gt;% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE) %&gt;% 
  tokens_tolower() %&gt;% 
  tokens_replace(lemma_data$inflected_form, lemma_data$lemma, valuetype = &quot;fixed&quot;) %&gt;% 
  tokens_remove(pattern = stopwords_extended, padding = T)

sotu_collocations &lt;- textstat_collocations(corpus_tokens, min_count = 25)
sotu_collocations &lt;- sotu_collocations[1:250, ]

corpus_tokens &lt;- tokens_compound(corpus_tokens, sotu_collocations)</code></pre>
<div id="model-calculation" class="section level1">
<h1><span class="header-section-number">1</span> Model calculation</h1>
<p>After the preprocessing, we have two corpus objects: <code>processedCorpus</code>, on which we calculate an LDA topic model <span class="citation">[1]</span>. To this end, stopwords were removed, words were stemmed and converted to lowercase letters and special characters were removed. The second Corpus object <code>corpus</code> serves to be able to view the original texts and thus to facilitate a qualitative control of the topic model results.</p>
<p>We now calculate a topic model on the <code>processedCorpus</code>. For this purpose, a DTM of the corpus is created. In this case, we only want to consider terms that occur with a certain minimum frequency in the body. This is primarily used to speed up the model calculation.</p>
<pre class="r"><code># Create DTM, but remove terms which occur in less than 1% of all documents
DTM &lt;- corpus_tokens %&gt;% 
  tokens_remove(&quot;&quot;) %&gt;%
  dfm() %&gt;% 
  dfm_trim(min_docfreq = 0.01, max_docfreq = Inf, docfreq_type = &quot;prop&quot;)

# have a look at the number of documents and terms in the matrix
dim(DTM)</code></pre>
<pre><code>## [1]  233 9389</code></pre>
<p>For topic modeling not only language specific stop words may beconsidered as uninformative, but also domain specific terms. We remove 10 of the most frequent terms to improve the modeling.</p>
<pre class="r"><code>top10_terms &lt;- c( &quot;unite_state&quot;, &quot;past_year&quot;, &quot;year_ago&quot;, &quot;year_end&quot;, &quot;government&quot;, &quot;state&quot;, &quot;country&quot;, &quot;year&quot;, &quot;make&quot;, &quot;seek&quot;)

DTM &lt;- DTM[, !(colnames(DTM) %in% top10_terms)]

# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx &lt;- rowSums(DTM) &gt; 0
DTM &lt;- DTM[sel_idx, ]
textdata &lt;- textdata[sel_idx, ]</code></pre>
<p>As an unsupervised machine learning method, topic models are suitable for the exploration of data. The calculation of topic models aims to determine the proportionate composition of a fixed number of topics in the documents of a collection. It is useful to experiment with different parameters in order to find the most suitable parameters for your own analysis needs.</p>
<p>For parameterized models such as Latent Dirichlet Allocation (LDA), the number of topics <code>K</code> is the most important parameter to define in advance. How an optimal <code>K</code> should be selected depends on various factors. If <code>K</code> is too small, the collection is divided into a few very general semantic contexts. If <code>K</code> is too large, the collection is divided into too many topics of which some may overlap and others are hardly interpretable.</p>
<p>For our first analysis we choose a thematic “resolution” of <code>K = 20</code> topics. In contrast to a resolution of 100 or more, this number of topics can be evaluated qualitatively very easy. We also set the seed for the random number generator to ensure reproducible results between repeated model inferences.</p>
<pre class="r"><code># load package topicmodels
require(topicmodels)
# number of topics
K &lt;- 20
# compute the LDA model, inference via n iterations of Gibbs sampling
topicModel &lt;- LDA(DTM, K, method=&quot;Gibbs&quot;, control=list(iter = 500, seed = 1, verbose = 25))</code></pre>
<p>Depending on the size of the vocabulary, the collection size and the number K, the inference of topic models can take a very long time. This calculation may take several minutes. If it takes too long, reduce the vocabulary in the DTM by increasing the minimum frequency in the previous step.</p>
<p>The topic model inference results in two (approximate) posterior probability distributions: a distribution <code>theta</code> over K topics within each document and a distribution <code>beta</code> over V terms within each topic, where V represents the length of the vocabulary of the collection (V = 9379). Let’s take a closer look at these results:</p>
<pre class="r"><code># have a look a some of the results (posterior distributions)
tmResult &lt;- posterior(topicModel)
# format of the resulting object
attributes(tmResult)</code></pre>
<pre><code>## $names
## [1] &quot;terms&quot;  &quot;topics&quot;</code></pre>
<pre class="r"><code>ncol(DTM)                # lengthOfVocab</code></pre>
<pre><code>## [1] 9379</code></pre>
<pre class="r"><code># topics are probability distribtions over the entire vocabulary
beta &lt;- tmResult$terms   # get beta from results
dim(beta)                # K distributions over ncol(DTM) terms</code></pre>
<pre><code>## [1]   20 9379</code></pre>
<pre class="r"><code>rowSums(beta)            # rows in beta sum to 1</code></pre>
<pre><code>##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 
##  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1</code></pre>
<pre class="r"><code>nrow(DTM)               # size of collection</code></pre>
<pre><code>## [1] 233</code></pre>
<pre class="r"><code># for every document we have a probability distribution of its contained topics
theta &lt;- tmResult$topics 
dim(theta)               # nDocs(DTM) distributions over K topics</code></pre>
<pre><code>## [1] 233  20</code></pre>
<pre class="r"><code>rowSums(theta)[1:10]     # rows in theta sum to 1</code></pre>
<pre><code>##  1  2  3  4  5  6  7  8  9 10 
##  1  1  1  1  1  1  1  1  1  1</code></pre>
<p>Let’s take a look at the 10 most likely terms within the term probabilities <code>beta</code> of the inferred topics (only the first 8 are shown below).</p>
<pre class="r"><code>terms(topicModel, 10)</code></pre>
<pre><code>##       Topic 1       Topic 2     Topic 3          Topic 4       Topic 5    Topic 6     Topic 7    Topic 8      
##  [1,] &quot;increase&quot;    &quot;public&quot;    &quot;administration&quot; &quot;gold&quot;        &quot;congress&quot; &quot;america&quot;   &quot;program&quot;  &quot;man&quot;        
##  [2,] &quot;law&quot;         &quot;bank&quot;      &quot;policy&quot;         &quot;amount&quot;      &quot;act&quot;      &quot;american&quot;  &quot;congress&quot; &quot;law&quot;        
##  [3,] &quot;report&quot;      &quot;subject&quot;   &quot;work&quot;           &quot;silver&quot;      &quot;treaty&quot;   &quot;nation&quot;    &quot;american&quot; &quot;work&quot;       
##  [4,] &quot;congress&quot;    &quot;interest&quot;  &quot;effort&quot;         &quot;treasury&quot;    &quot;duty&quot;     &quot;terrorist&quot; &quot;increase&quot; &quot;good&quot;       
##  [5,] &quot;work&quot;        &quot;duty&quot;      &quot;development&quot;    &quot;issue&quot;       &quot;subject&quot;  &quot;freedom&quot;   &quot;billion&quot;  &quot;business&quot;   
##  [6,] &quot;indian&quot;      &quot;general&quot;   &quot;system&quot;         &quot;bond&quot;        &quot;present&quot;  &quot;great&quot;     &quot;federal&quot;  &quot;corporation&quot;
##  [7,] &quot;legislation&quot; &quot;attention&quot; &quot;continue&quot;       &quot;circulation&quot; &quot;citizen&quot;  &quot;act&quot;       &quot;nation&quot;   &quot;power&quot;      
##  [8,] &quot;secretary&quot;   &quot;system&quot;    &quot;national&quot;       &quot;note&quot;        &quot;claim&quot;    &quot;iraq&quot;      &quot;energy&quot;   &quot;nation&quot;     
##  [9,] &quot;land&quot;        &quot;time&quot;      &quot;support&quot;        &quot;present&quot;     &quot;time&quot;     &quot;world&quot;     &quot;world&quot;    &quot;great&quot;      
## [10,] &quot;department&quot;  &quot;measure&quot;   &quot;international&quot;  &quot;pay&quot;         &quot;part&quot;     &quot;fight&quot;     &quot;continue&quot; &quot;condition&quot;</code></pre>
<p>For the next steps, we want to give the topics more descriptive names than just numbers. Therefore, we simply concatenate the five most likely terms of each topic to a string that represents a pseudo-name for each topic.</p>
<pre class="r"><code>top5termsPerTopic &lt;- terms(topicModel, 5)
topicNames &lt;- apply(top5termsPerTopic, 2, paste, collapse=&quot; &quot;)</code></pre>
</div>
<div id="visualization-of-words-and-topics" class="section level1">
<h1><span class="header-section-number">2</span> Visualization of Words and Topics</h1>
<p>Although wordclouds may not be optimal for scientific purposes they can provide a quick visual overview of a set of terms. Let’s look at some topics as wordcloud.</p>
<p>In the following code, you can change the variable <strong>topicToViz</strong> with values between 1 and 20 to display other topics.</p>
<pre class="r"><code>require(wordcloud2)
# visualize topics as word cloud
topicToViz &lt;- 11 # change for your own topic of interest
topicToViz &lt;- grep(&#39;mexico&#39;, topicNames)[1] # Or select a topic by a term contained in its name
# select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top40terms &lt;- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]
words &lt;- names(top40terms)
# extract the probabilites of each of the 40 terms
probabilities &lt;- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]
# visualize the terms as wordcloud
wordcloud2(data.frame(words, probabilities), shuffle = FALSE, size = 0.8)</code></pre>
<div id="htmlwidget-b6d99c97ec4a1f7688e8" style="width:384px;height:384px;" class="wordcloud2 html-widget"></div>
<script type="application/json" data-for="htmlwidget-b6d99c97ec4a1f7688e8">{"x":{"word":["mexico","war","texas","army","amount","policy","revenue","large","peace","treasury","territory","land","great","price","establish","nation","secretary","mexican","increase","recommend","measure","people","california","loan","world","american","mile","command","system","reduce","month","remain","numb","term","rate","coast","military","extend","oregon","estimate"],"freq":[0.0283255490555117,0.0176229254565907,0.00997819431450424,0.00903170379215068,0.00834003764120001,0.00783038889839424,0.00753916104536238,0.00742995060047543,0.00710231926581458,0.00655626704137983,0.00648346007812187,0.00630144266997696,0.00601021481694509,0.00582819740880018,0.00568258348228424,0.00564618000065526,0.00553696955576832,0.00550056607413933,0.00542775911088136,0.0051365312578495,0.0046996894783017,0.00462688251504374,0.00462688251504374,0.00444486510689882,0.00444486510689882,0.00437205814364086,0.00419004073549594,0.00415363725386696,0.00404442680898001,0.00400802332735103,0.00393521636409306,0.00389881288246408,0.00378960243757713,0.00371679547431916,0.0036439885110612,0.00360758502943221,0.00357118154780323,0.00353477806617425,0.00353477806617425,0.00349837458454526],"fontFamily":"Segoe UI","fontWeight":"bold","color":"random-dark","minSize":0,"weightFactor":5083.74964657499,"backgroundColor":"white","gridSize":0,"minRotation":-0.785398163397448,"maxRotation":0.785398163397448,"shuffle":false,"rotateRatio":0.4,"shape":"circle","ellipticity":0.65,"figBase64":null,"hover":null},"evals":[],"jsHooks":{"render":[{"code":"function(el,x){\n                        console.log(123);\n                        if(!iii){\n                          window.location.reload();\n                          iii = False;\n\n                        }\n  }","data":null}]}}</script>
<p>Let us now look more closely at the distribution of topics within individual documents. To this end, we visualize the distribution in 3 sample documents.</p>
<p>Let us first take a look at the contents of three sample documents:</p>
<pre class="r"><code>exampleIds &lt;- c(2, 100, 200)
cat(sotu_corpus[exampleIds[1]])
cat(sotu_corpus[exampleIds[2]])
cat(sotu_corpus[exampleIds[3]])</code></pre>
<pre><code>## 2: Fellow-Citizens of the Senate and House of Representatives:
## 
## In meeting you again I feel much satisfaction in being able to repeat my
## congratulations on the favorable prospects which continue to distinguish
## our public affairs. The abundant fruits of another year have blessed our
## country with plenty and with the means of a flourishing commerce.
## 
## The progress of public credit is witnessed by a consi...</code></pre>
<pre><code>## 100: To the Congress of the United States:
## 
## As you assemble for the discharge of the duties you have assumed as the
## representatives of a free and generous people, your meeting is marked by an
## interesting and impressive incident. With the expiration of the present
## session of the Congress the first century of our constitutional existence
## as a nation will be completed.
## 
## Our survival for one hundred years ...</code></pre>
<pre><code>## 200: Mr. Speaker, Mr. President, distinguished Members of Congress, honored
## guests, and fellow citizens:
## 
## May I congratulate all of you who are Members of this historic 100th
## Congress of the United States of America. In this 200th anniversary year of
## our Constitution, you and I stand on the shoulders of giants--men whose
## words and deeds put wind in the sails of freedom. However, we must always
## remember...</code></pre>
<p>After looking into the documents, we visualize the topic distributions within the documents.</p>
<pre class="r"><code># load libraries for visualization
library(&quot;reshape2&quot;)
library(&quot;ggplot2&quot;)
N &lt;- length(exampleIds)
# get topic proportions form example documents
topicProportionExamples &lt;- theta[exampleIds,]
colnames(topicProportionExamples) &lt;- topicNames
vizDataFrame &lt;- melt(cbind(data.frame(topicProportionExamples), document = factor(1:N)), variable.name = &quot;topic&quot;, id.vars = &quot;document&quot;)  

ggplot(data = vizDataFrame, aes(topic, value, fill = document), ylab = &quot;proportion&quot;) + 
  geom_bar(stat=&quot;identity&quot;) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  
  coord_flip() +
  facet_wrap(~ document, ncol = N)</code></pre>
<p><img src="Tutorial_6_Topic_Models_files/figure-html/unnamed-chunk-8-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
<div id="topic-distributions" class="section level1">
<h1><span class="header-section-number">3</span> Topic distributions</h1>
<p>The figure above shows how topics within a document are distributed according to the model. In the current model all three documents show at least a small percentage of each topic. However, two to three topics dominate each document.</p>
<p>The topic distribution within a document can be controlled with the <em>Alpha</em>-parameter of the model. Higher alpha priors for topics result in an even distribution of topics within a document. Low alpha priors ensure that the inference process distributes the probability mass on a few topics for each document.</p>
<p>In the previous model calculation the alpha-prior was automatically estimated in order to fit to the data (highest overall probability of the model). However, this automatic estimate does not necessarily correspond to the results that one would like to have as an analyst. Depending on our analysis interest, we might be interested in a more peaky/more even distribution of topics in the model.</p>
<p>Now let us change the alpha prior to a lower value to see how this affects the topic distributions in the model.</p>
<pre class="r"><code># see alpha from previous model
attr(topicModel, &quot;alpha&quot;) </code></pre>
<pre><code>## [1] 2.5</code></pre>
<pre class="r"><code>topicModel2 &lt;- LDA(DTM, K, method=&quot;Gibbs&quot;, control=list(iter = 500, verbose = 25, seed = 1, alpha = 0.2))
tmResult &lt;- posterior(topicModel2)
theta &lt;- tmResult$topics
beta &lt;- tmResult$terms
topicNames &lt;- apply(terms(topicModel2, 5), 2, paste, collapse = &quot; &quot;)  # reset topicnames</code></pre>
<p>Now visualize the topic distributions in the three documents again. What are the differences in the distribution structure?</p>
<pre class="r"><code># get topic proportions form example documents
topicProportionExamples &lt;- theta[exampleIds,]
colnames(topicProportionExamples) &lt;- topicNames
vizDataFrame &lt;- melt(cbind(data.frame(topicProportionExamples), document = factor(1:N)), variable.name = &quot;topic&quot;, id.vars = &quot;document&quot;)  

ggplot(data = vizDataFrame, aes(topic, value, fill = document), ylab = &quot;proportion&quot;) + 
  geom_bar(stat=&quot;identity&quot;) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  
  coord_flip() +
  facet_wrap(~ document, ncol = N)</code></pre>
<p><img src="Tutorial_6_Topic_Models_files/figure-html/unnamed-chunk-10-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
<div id="topic-ranking" class="section level1">
<h1><span class="header-section-number">4</span> Topic ranking</h1>
<p>First, we try to get a more meaningful order of top terms per topic by re-ranking them with a specific score <span class="citation">[2]</span>. The idea of re-ranking terms is similar to the idea of TF-IDF. The more a term appears in top levels w.r.t. its probability, the less meaningful it is to describe the topic. Hence, the scoring favors less general, more specific terms to describe a topic.</p>
<pre class="r"><code># re-rank top topic terms for topic names
topicNames &lt;- apply(lda::top.topic.words(beta, 5, by.score = T), 2, paste, collapse = &quot; &quot;)</code></pre>
<p>What are the defining topics within a collection? There are different approaches to find out which can be used to bring the topics into a certain order.</p>
<p><strong>Approach 1:</strong> We sort topics according to their probability within the entire collection:</p>
<pre class="r"><code># What are the most probable topics in the entire collection?
topicProportions &lt;- colSums(theta) / nrow(DTM)  # mean probablities over all paragraphs
names(topicProportions) &lt;- topicNames     # assign the topic names we created before
sort(topicProportions, decreasing = TRUE) # show summed proportions in decreased order</code></pre>
<pre><code>##  [1] &quot;0.20355 : congress time treaty act subject&quot;                &quot;0.12296 : congress american america good nation&quot;          
##  [3] &quot;0.12152 : public people great power subject&quot;               &quot;0.07574 : object nation commerce spain war&quot;               
##  [5] &quot;0.07112 : nation great world life time&quot;                    &quot;0.06418 : work job child america tonight&quot;                 
##  [7] &quot;0.05948 : program economic federal expenditure congress&quot;   &quot;0.05331 : world defense peace strength effort&quot;            
##  [9] &quot;0.03185 : department american court commission board&quot;      &quot;0.02459 : program energy administration policy continue&quot;  
## [11] &quot;0.02267 : terrorist america iraq iraqi terror&quot;             &quot;0.02087 : island international cuba july commission&quot;      
## [13] &quot;0.02018 : person report legislation claim show&quot;            &quot;0.01849 : war fight enemy man victory&quot;                    
## [15] &quot;0.0183 : man corporation law good business&quot;                &quot;0.01611 : indian report increase work total&quot;              
## [17] &quot;0.01604 : constitution union territory congress president&quot; &quot;0.01324 : gold bond silver coin amount&quot;                   
## [19] &quot;0.01299 : mexico texas war mexican army&quot;                   &quot;0.01279 : bank currency deposit credit public_money&quot;</code></pre>
<p>We recognize some topics that are way more likely to occur in the corpus than others. These describe rather general thematic coherences. Other topics correspond more to specific contents.</p>
<p><strong>Approach 2:</strong> We count how often a topic appears as a primary topic within a paragraph This method is also called Rank-1.</p>
<pre class="r"><code>countsOfPrimaryTopics &lt;- rep(0, K)
names(countsOfPrimaryTopics) &lt;- topicNames
for (i in 1:nrow(DTM)) {
  topicsPerDoc &lt;- theta[i, ] # select topic distribution for document i
  # get first element position from ordered list
  primaryTopic &lt;- order(topicsPerDoc, decreasing = TRUE)[1] 
  countsOfPrimaryTopics[primaryTopic] &lt;- countsOfPrimaryTopics[primaryTopic] + 1
}
sort(countsOfPrimaryTopics, decreasing = TRUE)</code></pre>
<pre><code>##  [1] &quot;85 : congress time treaty act subject&quot;               &quot;33 : congress american america good nation&quot;         
##  [3] &quot;19 : program economic federal expenditure congress&quot;  &quot;18 : nation great world life time&quot;                  
##  [5] &quot;17 : work job child america tonight&quot;                 &quot;15 : object nation commerce spain war&quot;              
##  [7] &quot;10 : world defense peace strength effort&quot;            &quot;8 : public people great power subject&quot;              
##  [9] &quot;7 : man corporation law good business&quot;               &quot;6 : terrorist america iraq iraqi terror&quot;            
## [11] &quot;4 : department american court commission board&quot;      &quot;3 : program energy administration policy continue&quot;  
## [13] &quot;3 : war fight enemy man victory&quot;                     &quot;2 : indian report increase work total&quot;              
## [15] &quot;2 : gold bond silver coin amount&quot;                    &quot;1 : constitution union territory congress president&quot;
## [17] &quot;0 : bank currency deposit credit public_money&quot;       &quot;0 : person report legislation claim show&quot;           
## [19] &quot;0 : mexico texas war mexican army&quot;                   &quot;0 : island international cuba july commission&quot;</code></pre>
<p>We see that sorting topics by the Rank-1 method places topics with rather specific thematic coherences in upper ranks of the list.</p>
<p>This sorting of topics can be used for further analysis steps such as the semantic interpretation of topics found in the collection, the analysis of time series of the most important topics or the filtering of the original collection based on specific sub-topics.</p>
</div>
<div id="filtering-documents" class="section level1">
<h1><span class="header-section-number">5</span> Filtering documents</h1>
<p>The fact that a topic model conveys of topic probabilities for each document, resp. paragraph in our case, makes it possible to use it for thematic filtering of a collection. As filter we select only those documents which exceed a certain threshold of their probability value for certain topics (for example, each document which contains topic <code>X</code> to more than <code>Y</code> percent).</p>
<p>In the following, we will select documents based on their topic content and display the resulting document quantity over time.</p>
<pre class="r"><code>topicToFilter &lt;- 6  # you can set this manually ...
# ... or have it selected by a term in the topic name
topicToFilter &lt;- grep(&#39;mexico &#39;, topicNames)[1] 
topicThreshold &lt;- 0.1 # minimum share of content must be attributed to the selected topic
selectedDocumentIndexes &lt;- (theta[, topicToFilter] &gt;= topicThreshold)
filteredCorpus &lt;- sotu_corpus %&gt;% corpus_subset(subset = selectedDocumentIndexes)

# show length of filtered corpus
filteredCorpus</code></pre>
<pre><code>## Corpus consisting of 5 documents.
## 56 :
## &quot;To the Senate and House of Representatives of the United Sta...&quot;
## 
## 57 :
## &quot;Fellow-Citizens of the Senate and of the House of Representa...&quot;
## 
## 58 :
## &quot;Fellow-Citizens of the Senate and of the House of Representa...&quot;
## 
## 59 :
## &quot;Fellow-Citizens of the Senate and of the House of Representa...&quot;
## 
## 60 :
## &quot;Fellow-Citizens of the Senate and of the House of Representa...&quot;</code></pre>
<p>Our filtered corpus contains 5 documents related to the topic 17 to at least 10 %.</p>
</div>
<div id="topic-proportions-over-time" class="section level1">
<h1><span class="header-section-number">6</span> Topic proportions over time</h1>
<p>In a last step, we provide a distant view on the topics in the data over time. For this, we aggregate mean topic proportions per decade of all SOTU speeches. These aggregated topic proportions can then be visualized, e.g. as a bar plot.</p>
<pre class="r"><code># append decade information for aggregation
textdata$decade &lt;- paste0(substr(textdata$date, 0, 3), &quot;0&quot;)
# get mean topic proportions per decade
topic_proportion_per_decade &lt;- aggregate(theta, by = list(decade = textdata$decade), mean)
# set topic names to aggregated columns
colnames(topic_proportion_per_decade)[2:(K+1)] &lt;- topicNames

# reshape data frame
vizDataFrame &lt;- melt(topic_proportion_per_decade, id.vars = &quot;decade&quot;)

# plot topic proportions per deacde as bar plot
require(pals)
ggplot(vizDataFrame, aes(x=decade, y=value, fill=variable)) + 
  geom_bar(stat = &quot;identity&quot;) + ylab(&quot;proportion&quot;) + 
  scale_fill_manual(values = paste0(alphabet(20), &quot;FF&quot;), name = &quot;decade&quot;) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))</code></pre>
<p><img src="Tutorial_6_Topic_Models_files/figure-html/unnamed-chunk-16-1.png" width="864" style="display: block; margin: auto;" /></p>
<p>The visualization shows that topics around the relation between the federal government and the states as well as inner conflicts clearly dominate the first decades. Security issues and the economy are the most important topics of recent SOTU addresses.</p>
<p>For more details about topic modeling and some best practice advise, see also <span class="citation">[3]</span>.</p>
</div>
<div id="optional-exercises" class="section level1">
<h1><span class="header-section-number">7</span> Optional exercises</h1>
<ol style="list-style-type: decimal">
<li>Create a list of all documents that contain a share of <em>war</em>-related topics of at least 50 % (possible approach: sub-select topics which contain the term <em>war</em> among the top 15 terms).</li>
</ol>
<pre><code>## Selected topics:</code></pre>
<pre><code>##                                 10                                 17                                 18 
## &quot;object nation commerce spain war&quot;    &quot;mexico texas war mexican army&quot;     &quot;nation great world life time&quot; 
##                                 19 
##      &quot;war fight enemy man victory&quot;</code></pre>
<pre><code>## Beginnings of filtered speeches:</code></pre>
<pre><code>## Corpus consisting of 12 documents.
## 24 :
## &quot;Fellow-Citizens of the Senate and House of Representatives: ...&quot;
## 
## 25 :
## &quot;Fellow-Citizens of the Senate and House of Representatives: ...&quot;
## 
## 26 :
## &quot;Fellow-Citizens of the Senate and House of Representatives: ...&quot;
## 
## 125 :
## &quot;Gentlemen of the Congress: In pursuance of my constitutional...&quot;
## 
## 126 :
## &quot;GENTLEMEN OF THE CONGRESS: The session upon which you are no...&quot;
## 
## 127 :
## &quot;GENTLEMEN OF THE CONGRESS: Since I last had the privilege of...&quot;
## 
## [ reached max_ndoc ... 6 more documents ]</code></pre>
<pre><code>## Metadata of filtered speeches:</code></pre>
<pre><code>##     doc_id             president       date decade topic_share
## 24      24         James Madison 1812-11-04   1810       0.553
## 25      25         James Madison 1813-12-07   1810       0.603
## 26      26         James Madison 1814-09-20   1810       0.636
## 125    125        Woodrow Wilson 1913-12-02   1910       0.525
## 126    126        Woodrow Wilson 1914-12-08   1910       0.647
## 127    127        Woodrow Wilson 1915-12-07   1910       0.633
## 129    129        Woodrow Wilson 1917-12-04   1910       0.705
## 130    130        Woodrow Wilson 1918-12-02   1910       0.654
## 153    153 Franklin D. Roosevelt 1942-01-06   1940       0.607
## 154    154 Franklin D. Roosevelt 1943-01-07   1940       0.655
## 155    155 Franklin D. Roosevelt 1944-01-11   1940       0.525
## 156    156 Franklin D. Roosevelt 1945-01-06   1940       0.579</code></pre>
<ol start="2" style="list-style-type: decimal">
<li><p>Repeat the exercises with a different K value (e.g., K = 5, 30, 50). What do you think of the results?</p></li>
<li><p>Split the original text source into paragraphs (e.g. use <code>corpus_reshape(x, to = "paragraphs")</code>) and compute a topic model on paragraphs instead of full speeches. How does the smaller context unit affect the result?</p></li>
</ol>
<pre><code>## Length of the corpus split into paragraphs:</code></pre>
<pre><code>## [1] 21611</code></pre>
<p><img src="Tutorial_6_Topic_Models_files/figure-html/ex3-1.png" width="672" /></p>
<ol start="4" style="list-style-type: decimal">
<li>Use the LDAvis package by <span class="citation">[4]</span> to visualize the latest model you computed.</li>
</ol>
<pre class="r"><code># LDAvis browser
library(LDAvis)
library(&quot;tsne&quot;)
svd_tsne &lt;- function(x) tsne(svd(x)$u)
json &lt;- createJSON(
  phi = tmResult$beta, 
  theta = tmResult$theta, 
  doc.length = rowSums(DTM), 
  vocab = colnames(DTM), 
  term.frequency = colSums(DTM),
  mds.method = svd_tsne,
  plot.opts = list(xlab=&quot;&quot;, ylab=&quot;&quot;)
)
serVis(json)</code></pre>
<div class="figure">
<img src="resources/ldavis.PNG" alt="" />
<p class="caption">LDAvis example</p>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-blei_latent_2003">
<p>1. Blei, D.M., Ng, A.Y., Jordan, M.I.: Latent dirichlet allocation. The Journal of Machine Learning Research. 3, 993–1022 (2003).</p>
</div>
<div id="ref-Chang.2012">
<p>2. Chang, J.: Lda: Collapsed gibbs sampling methods for topic models. (2012).</p>
</div>
<div id="ref-Maier.2018">
<p>3. Maier, D., Waldherr, A., Miltner, P., Wiedemann, G., Niekler, A., Keinert, A., Pfetsch, B., Heyer, G., Reber, U., Häussler, T., Schmid-Petri, H., Adam, S.: Applying lda topic modeling in communication research: Toward a valid and reliable methodology. Communication Methods and Measures. 12, 93–118 (2018).</p>
</div>
<div id="ref-sievert_ldavis:_2014">
<p>4. Sievert, C., Shirley, K.E.: LDAvis: A method for visualizing and interpreting topics. In: Proceedings of the workshop on interactive language learning, visualization, and interfaces. pp. 63–70 (2014).</p>
</div>
</div>
</div>

<p>2020, Andreas Niekler and Gregor Wiedemann. GPLv3. <a href="https://tm4ss.github.io">tm4ss.github.io</a></p>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
